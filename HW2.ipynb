{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7199f587",
   "metadata": {},
   "source": [
    "# Homework 2: Stochastic Gradient Descent\n",
    "\n",
    "```{warning}\n",
    "The submission of the homeworks has **NO** deadline. You can submit them whenever you want, on Virtuale. You are only required to upload it on Virtuale **BEFORE** your exam session, since the Homeworks will be a central part of the oral exam. \n",
    "\n",
    "You are asked to submit the homework as one of the two, following modalities:\n",
    "* A PDF (or Word) document, containing screenshoots of code snippets, screeshots of the results generated by your code, and a brief comment on the obtained results.\n",
    "* A Python Notebook (i.e. a `.ipynb` file), with cells containing the code required to solve the indicated exercises, alternated with a brief comment on the obtained results in the form of a markdown cell. We remark that the code **SHOULD NOT** be runned during the exam, but the student is asked to enter the exam with all the programs **already executed**, with the results clearly visible on the screen.\n",
    "\n",
    "Joining the oral exam with a non-executed code OR without a PDF file with the obtained results visible on that, will cause the student to be rejected.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e237357",
   "metadata": {},
   "source": [
    "## Exercise 1: SGD vs GD on a Simple 1D Regression Problem\n",
    "\n",
    "Consider the synthetic dataset\n",
    "\n",
    "$$\n",
    "x^{(i)} = \\frac{i}{N},\\qquad\n",
    "y^{(i)} = 2x^{(i)} + 1 + \\varepsilon^{(i)}, \\qquad \\varepsilon^{(i)}\\sim\\mathcal N(0,0.01),\n",
    "$$\n",
    "\n",
    "with $N = 200$.\n",
    "We model the data with a linear function:\n",
    "\n",
    "$$\n",
    "f_\\Theta(x) = \\Theta_0 + \\Theta_1 x = \\Theta^T \\tilde{x},\n",
    "$$\n",
    "\n",
    "if we define $\\tilde{x} = [1, x]$ as we did during the lab session.\n",
    "\n",
    "\n",
    "1. Implement the MSE loss:\n",
    "   \n",
    "   $$\n",
    "   \\mathcal L(\\Theta)=\\frac{1}{N}\\sum_{i=1}^N (f_\\Theta(x^{(i)}) - y^{(i)})^2.\n",
    "   $$\n",
    "\n",
    "\n",
    "2. Implement **full GD** and **SGD** (mini-batch) using batch sizes:\n",
    "   - $N_{\\text{batch}} = 1$,\n",
    "   - $N_{\\text{batch}} = 10$,\n",
    "   - $N_{\\text{batch}} = 50$,\n",
    "   - $N_{\\text{batch}} = N$ (this recovers GD).\n",
    "\n",
    "3. Plot for each method:\n",
    "   - the loss curve (loss vs epoch),\n",
    "   - the trajectory of parameters $(\\Theta_0,\\Theta_1)$ in the 2D parameter space. This is similar to what you did in the previous homework: simply plot the value of $\\Theta_0^{(k)}$ and $\\Theta_1^{(k)}$ for all the $k$s in a 2-dimensional plot.\n",
    "\n",
    "4. Discuss:\n",
    "   - Why GD is smooth but slow for large $N$,\n",
    "   - Why SGD is noisy but progresses faster,\n",
    "   - How batch size affects the noise level and convergence stability.\n",
    "\n",
    "## Exercise 2: Variance of the Stochastic Gradient (1D Experiment)\n",
    "\n",
    "Fix a parameter vector $\\Theta$, and repeatedly draw random mini-batches of the same size.\n",
    "\n",
    "1. Choose batch sizes:\n",
    "   \n",
    "   $$\n",
    "   N_{\\text{batch}} \\in \\{1, 5, 20, N\\}.\n",
    "   $$\n",
    "\n",
    "\n",
    "2. At the **same** $\\Theta$, compute:\n",
    "   \n",
    "   $$\n",
    "   g_k = \\nabla_\\Theta \\mathcal L(\\Theta; \\mathcal M_k)\n",
    "   $$\n",
    "\n",
    "   for 100 randomly sampled batches $\\mathcal M_k$.\n",
    "\n",
    "3. For each batch size, compute the empirical variance:\n",
    "   \n",
    "   $$\n",
    "   \\mathrm{Var}(g) = \\frac{1}{100}\\sum_{k=1}^{100} \\|g_k - \\bar g\\|^2,\n",
    "   $$\n",
    "\n",
    "   where $\\bar{g}$ is the average of the $g_k$s, defined as:\n",
    "\n",
    "   $$\n",
    "   \\bar{g} = \\frac{1}{100} \\sum_{k=1}^{100} g_k.\n",
    "   $$\n",
    "\n",
    "\n",
    "4. Plot the variance as a function of the batch size.\n",
    "\n",
    "5. Comment:\n",
    "   - Why the variance decreases with larger batches,\n",
    "   - Why SGD becomes more stable as $N_{\\text{batch}}$ increases,\n",
    "   - The trade-off between stability and computational cost.\n",
    "\n",
    "## Exercise 3: SGD in 2D\n",
    "\n",
    "We now study SGD on the 2D non-convex function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta_1,\\Theta_2) = (\\Theta_1^2 - 1)^2 + 10(\\Theta_2 - \\Theta_1^2)^2.\n",
    "$$\n",
    "\n",
    "This function has:\n",
    "- two valleys,\n",
    "- multiple stationary points,\n",
    "- strong curvature differences.\n",
    "\n",
    "1. Treat $\\Theta = (\\Theta_1,\\Theta_2)$ as a “parameter vector” updated by SGD:\n",
    "   \n",
    "   $$\n",
    "   \\Theta_{k+1} = \\Theta_k - \\eta\\, g_k,\n",
    "   $$\n",
    "\n",
    "   where the ``gradient batch'' $g_k$ is simulated by adding noise to the gradient:\n",
    "   \n",
    "   $$\n",
    "   g_k = \\nabla \\mathcal{L}(\\Theta_k) + \\varepsilon_k, \\qquad \\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2I),\n",
    "   $$\n",
    "\n",
    "   where $\\sigma^2$ is called **noise level** and represent the variance of the noise. Try different values of $\\sigma^2$ to answer the following questions. *Note: $\\sigma^2$ should always be lower than 1*. \n",
    "\n",
    "2. Plot:\n",
    "   - level sets of $\\mathcal{L}(\\Theta_1,\\Theta_2)$,\n",
    "   - trajectories of SGD for different noise levels and step sizes.\n",
    "\n",
    "3. Discuss:\n",
    "   - How noise helps escape shallow minima or bad regions,\n",
    "   - How too much noise prevents convergence,\n",
    "\n",
    "## Exercise 4: An ML Project with SGD\n",
    "\n",
    "For the final exercise, you will train a simple machine learning model using SGD on a real prediction task. \n",
    "To begin, download the Kaggle dataset: https://www.kaggle.com/datasets/mirichoi0218/insurance.\n",
    "\n",
    "It contains approximately 1300 samples, and its associated task is to predict **individual medical insurance cost** based on numerical features:\n",
    "\n",
    "- `age`\n",
    "- `bmi`\n",
    "- `children`  \n",
    "\n",
    "The task is:\n",
    "\n",
    "$$\n",
    "\\text{charges} \\approx f_\\Theta(\\text{age},\\text{bmi},\\text{children}).\n",
    "$$\n",
    "\n",
    "We use a simple linear model:\n",
    "\n",
    "$$\n",
    "f_\\Theta(x) = \\Theta^T \\tilde{x}.\n",
    "$$\n",
    "\n",
    "1. Load & preprocess the dataset\n",
    "   - Download `insurance.csv` from Kaggle.\n",
    "   - Select numerical columns:  \n",
    "     `[\"age\", \"bmi\", \"children\"]`.\n",
    "   - Standardize each feature (mean 0, variance 1).\n",
    "   - Standardize the target `\"charges\"`.\n",
    "   - Add a bias column.\n",
    "  \n",
    "2. Consider the MSE loss:\n",
    "\n",
    "   $$\n",
    "   \\mathcal{L}(\\Theta)=\\frac{1}{N}\\sum_{i=1}^N (\\Theta^T \\tilde{x}^{(i)} - \\tilde{y}^{(i)})^2.\n",
    "   $$\n",
    "\n",
    "   Implement:\n",
    "   - Full GD  \n",
    "   - SGD with batch sizes 1, 10, 50\n",
    "\n",
    "   Use a fixed learning rate $\\eta=10^{-2}$ and a fixed number of epochs (equivalently, a fixed amount of maximum iterations for GD) of your choice.\n",
    "\n",
    "3. Compare GD and SGD\n",
    "   For each method:\n",
    "\n",
    "   - Plot the loss vs epoch.\n",
    "   - Plot the L2 norm of the **full gradient** $\\|\\nabla \\mathcal{L}(\\Theta_k)\\|$ measured at the end of every epoch.\n",
    "   - Report the final learned parameters.\n",
    "\n",
    "4. Discuss:\n",
    "   - Why GD gives a smooth curve and SGD oscillates.\n",
    "   - Why larger batches reduce noise but cost more per iteration.\n",
    "   - Why all methods roughly converge to the same region.\n",
    "   - Why SGD is more suitable for large datasets, even when noisy."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
