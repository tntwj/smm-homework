{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7199f587",
   "metadata": {},
   "source": [
    "# Homework 1: Gradient Descent\n",
    "\n",
    "```{warning}\n",
    "The submission of the homeworks has **NO** deadline. You can submit them whenever you want, on Virtuale. You are only required to upload it on Virtuale **BEFORE** your exam session, since the Homeworks will be a central part of the oral exam. \n",
    "\n",
    "You are asked to submit the homework as one of the two, following modalities:\n",
    "* A PDF (or Word) document, containing screenshoots of code snippets, screeshots of the results generated by your code, and a brief comment on the obtained results.\n",
    "* A Python Notebook (i.e. a `.ipynb` file), with cells containing the code required to solve the indicated exercises, alternated with a brief comment on the obtained results in the form of a markdown cell. We remark that the code **SHOULD NOT** be runned during the exam, but the student is asked to enter the exam with all the programs **already executed**, with the results clearly visible on the screen.\n",
    "\n",
    "Joining the oral exam with a non-executed code OR without a PDF file with the obtained results visible on that, will cause the student to be rejected.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa11f5",
   "metadata": {},
   "source": [
    "## Exercise 1: GD on a 1D Function\n",
    "\n",
    "Consider the 1-dimensional function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta)  = (\\Theta - 3)^2 + 1.\n",
    "$$\n",
    "\n",
    "\n",
    "1. **Compute the Gradient** of $\\mathcal{L}(\\Theta)$ explicitly.\n",
    "\n",
    "2. **Implement the Gradient Descent** to optimize $\\mathcal{L}(\\Theta)$ following what we introduced on the theoretical sections.\n",
    "\n",
    "3. Test three different constant step sizes:\n",
    "   - $\\eta = 0.05$  \n",
    "   - $\\eta = 0.2$\n",
    "   - $\\eta = 1.0$\n",
    "\n",
    "4. For each choice:\n",
    "   - Plot the sequence $\\Theta^{(k)}$ on the real line (i.e. draw a line and represent, on top of it, the position of all the successive elements $\\Theta^{(k)}$).  \n",
    "   - Plot the function values $\\mathcal{L}(\\Theta^{(k)})$ vs iteration.  \n",
    "   - Comment on convergence, oscillations, and divergence.\n",
    "\n",
    "5. Relate your observations to the discussion in class about:\n",
    "   - step-size being too small / too large,\n",
    "   - the role of convexity,\n",
    "   - how the “just right” step size leads to fast convergence.\n",
    "\n",
    "*Hint:* This function is strictly convex with a unique minimizer at $\\Theta^* = 3$.\n",
    "\n",
    "## Exercise 2: Backtracking Line Search\n",
    "\n",
    "Consider the non-convex function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta)  = \\Theta^4 - 3\\Theta^2 + 2.\n",
    "$$\n",
    "\n",
    "\n",
    "1. Implement **Gradient Descent with Backtracking**, using the Armijo condition, considering the `backtracking(...)` function from class.\n",
    "   \n",
    "2. Test different initial points:\n",
    "   - $\\Theta_0 = -2$\n",
    "   - $\\Theta_0 = 0.5$\n",
    "   - $\\Theta_0 = 2$\n",
    "\n",
    "3. For each starting point, plot:\n",
    "   - the function curve $\\mathcal{L}(\\Theta)$ in 1D in the domain $[-3, 3]$,\n",
    "   - the trajectory of the iterates $\\Theta^{(k)}$ overlaid on the curve.\n",
    "\n",
    "4. Discuss:\n",
    "   - Why different initializations converge to different minima.  \n",
    "   - How backtracking automatically chooses a suitable step size at each iteration.  \n",
    "   - Situations where constant step size would fail.\n",
    "  \n",
    "\n",
    "## Exercise 3: GD in 2D\n",
    "\n",
    "Consider the quadratic function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta) = \\tfrac{1}{2}\\,\\Theta^T A\\,\\Theta,\n",
    "\\qquad\n",
    "A = \\begin{bmatrix}1 & 0 \\\\ 0 & 25\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "1. Implement Gradient Descent in 2D, as seen in class.\n",
    "   \n",
    "\n",
    "2. Plot the **level sets** of $L$ (using the helper code provided in the lecture notes) and overlay the iterates $\\Theta^{(k)}$ for:\n",
    "   - $\\eta = 0.02$,\n",
    "   - $\\eta = 0.05$ (borderline),\n",
    "   - $\\eta = 0.1$ (diverging).\n",
    "   To overlay the iterates on the plot, simply add a `plt.plot(...)` function taking as input the coordinates $\\Theta_1^{(k)}$ and $\\Theta_2^{(k)}$ for each $k$.\n",
    "\n",
    "3. Comment on:\n",
    "   - the elongated ellipses produced by ill-conditioning,\n",
    "   - the gradient direction compared to the level set lines,\n",
    "   - zig-zag behaviour for large condition numbers,\n",
    "   - the relation to the lecture discussion on slow convergence in narrow valleys.\n",
    "   \n",
    "\n",
    "## Exercise 4: Exact Line Search vs Backtracking\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\Theta) = \\frac{1}{2}\\Theta^T A \\Theta,\\qquad\n",
    "A=\\begin{bmatrix}5 & 0\\\\0 & 2\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "1. For GD with **exact line search** i.e. selecting the **optimal** step, you get:\n",
    "   \n",
    "   $$\n",
    "   \\eta_k^* = \\frac{g_k^T g_k}{g_k^T A\\,g_k},\n",
    "   \\qquad g_k = A\\,\\Theta^{(k)}.\n",
    "   $$\n",
    "   \n",
    "\n",
    "2. For GD with **backtracking**, use the implementation provided in your notes.\n",
    "\n",
    "3. Starting from $\\Theta_0 = (3,\\,3)^T$:\n",
    "   - Plot trajectories on the level-set map.  \n",
    "   - Plot the loss $\\mathcal{L}(\\Theta^{(k)})$ vs iteration for both methods.  \n",
    "\n",
    "4. Compare:\n",
    "   - speed of convergence,\n",
    "   - smoothness of step-sizes.\n",
    "\n",
    "## Exercise 5: Gradient Descent on the Rosenbrock Function\n",
    "\n",
    "The 2-dimensional Rosenbrock function is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal(\\Theta) = (1 - \\Theta_1)^2 + 100(\\Theta_2 - \\Theta_1^2)^2,\n",
    "$$\n",
    "\n",
    "with a unique global minimum at:\n",
    "\n",
    "$$\n",
    "\\Theta^* = (1,\\,1), \\qquad \\mathcal{L}(\\Theta_1^*,\\Theta_2y^*) = 0.\n",
    "$$\n",
    "\n",
    "\n",
    "Despite having only one minimum, its landscape is **extremely challenging**:\n",
    "- The valley is long and curved.\n",
    "- The condition number varies drastically across the region.\n",
    "- Gradients can point almost orthogonally to the valley direction.\n",
    "- Gradient Descent may zig-zag and make very slow progress toward the solution.\n",
    "\n",
    "This makes Rosenbrock a perfect testbed to study the difficulties of pure Gradient Descent.\n",
    "\n",
    "1. Implement the function and its gradient to run the Gradient Descent algorithm. Test its performance using both **constant step size** and the **backtracking algorithm**.\n",
    "\n",
    "2. Plot the level sets of $\\mathcal{L}$: use the standard 2D contour plotting approach practiced earlier.  \n",
    "Ensure that you plot:\n",
    "\n",
    "- A wide range (e.g. $\\Theta_1 \\in[-2,2]$, $\\Theta_2 \\in[-1,3]$)\n",
    "- A reasonable number of contour lines to visualize the narrow valley.\n",
    "\n",
    "1. Choose multiple initial points, testing at least the following four:\n",
    "- $\\Theta^{(0)} = (-1.5, \\; 2)$\n",
    "- $\\Theta^{(0)} = (-1, \\; 0)$\n",
    "- $\\Theta^{(0)} = (0, \\; 2)$\n",
    "- $\\Theta^{(0)} = (1.5, \\; 1.5)$\n",
    "\n",
    "Run both:\n",
    "- GD with constant step size (try $\\eta = 10^{-3}$, $10^{-4}$, $10^{-5}$),\n",
    "- GD with backtracking line search.\n",
    "\n",
    "2. Visualize and analyze the trajectories: For each initialization and each training method:\n",
    "\n",
    "   1. Plot the sequence of iterates $(\\Theta_1^{(k)}, \\Theta_2^{(k)})$ as a path on the level-set map.\n",
    "   2. Plot the loss curve $L(\\Theta^{(k)})$ vs iteration.\n",
    "   3. Observe and explain:\n",
    "      - whether the method enters the valley,\n",
    "      - how long it takes to “turn” correctly along the valley direction,\n",
    "      - whether the method zig-zags,\n",
    "      - whether step sizes become too small (with constant $\\eta$) or too large (divergence)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
